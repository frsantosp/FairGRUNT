{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-30T05:06:38.215351Z",
     "start_time": "2024-12-30T05:06:38.202250Z"
    }
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=True)\n",
    "        x = self.gc2(x, adj)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        nn.init.xavier_normal_(self.weight.data)\n",
    "        #nn.init.xavier_normal_(self.bias.data)\n",
    "        #self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)  # XW\n",
    "        output = torch.spmm(adj, support)  # AXW\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers, dropout=0.1):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "\n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.ReLU\n",
    "\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1):\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i + 1], bias=True))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            layer_list.append(('dropout_%d' % i, torch.nn.Dropout(dropout)))\n",
    "\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "\n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(OrderedDict(layer_list))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out  #F.softmax(out, dim=1)\n",
    "\n",
    "class ParamLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ParamLayer, self).__init__()\n",
    "        #self.param = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mm(x,self.weight)\n",
    "\n",
    "def dot_sim(x, y):\n",
    "    # Inner product similarity\n",
    "    ip_sim = torch.mm(x, y)\n",
    "    return ip_sim\n",
    "\n",
    "\n",
    "def load_data(path='data/', dataset='tagged', seed=80):\n",
    "    \"\"\"Load user network dataset (Tagged only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "    node_features = pd.read_csv(path + str(dataset) + '_features.csv', header=0)\n",
    "    if dataset == 'german':\n",
    "        node_features = node_features.drop(['PurposeOfLoan'], axis=1)\n",
    "    labels = torch.from_numpy(node_features['label'].to_numpy())  # label tensor\n",
    "    sensitive_attribute = torch.from_numpy(node_features['sens_2'].to_numpy()).type(torch.LongTensor)\n",
    "    #sensitive_attribute = torch.from_numpy(node_features['marital_status_indicator'].to_numpy()).type(\n",
    "    #    torch.LongTensor)  # gender tensor\n",
    "    #sensitive_attribute_1 = torch.from_numpy(node_features['sens_1'].to_numpy()).type(torch.LongTensor)\n",
    "    # last three columns are userIds, sensitives and labels\n",
    "    #node_features = node_features.drop(columns=['slobodny(a)', 'mam vazny vztah', 'zenaty (vydata)', 'rozvedeny(a)'])\n",
    "    features = node_features[node_features.columns[:-4]].to_numpy()\n",
    "        \n",
    "    print(node_features[node_features.columns[:-4]])\n",
    "    relations = pd.read_csv(path + str(dataset) + '_edges.csv', header=0)\n",
    "    # build graph\n",
    "    try:\n",
    "        adj = sp.coo_matrix((relations['weight'].to_numpy(), (relations['src'].to_numpy(),\n",
    "                                                              relations['dst'].to_numpy())),\n",
    "                            shape=(labels.shape[0], labels.shape[0]),\n",
    "                            dtype=np.float32)\n",
    "    except KeyError:\n",
    "        adj = sp.coo_matrix((np.ones(relations.shape[0]), (relations['src'].to_numpy(),\n",
    "                                                           relations['dst'].to_numpy())),\n",
    "                            shape=(labels.shape[0], labels.shape[0]),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = feature_norm(torch.FloatTensor(features)) # normalize feature matrix\n",
    "    aug_adj = aug_normalized_adjacency(sp.csr_matrix(adj))  # normalize adjacency matrix and add self loop\n",
    "    aug_adj = sparse_mx_to_torch_sparse_tensor(aug_adj)\n",
    "\n",
    "    # randomize dataset selection\n",
    "    random.seed(seed)\n",
    "    label_idx_0 = np.where(labels == 0)[0]\n",
    "    label_idx_1 = np.where(labels == 1)[0]\n",
    "\n",
    "    random.shuffle(label_idx_0)\n",
    "    random.shuffle(label_idx_1)\n",
    "\n",
    "    idx_train = np.append(label_idx_0[:int(0.6 * len(label_idx_0))], label_idx_1[:int(0.6 * len(label_idx_1))])\n",
    "    idx_val = np.append(label_idx_0[int(0.6 * len(label_idx_0)):int(0.8 * len(label_idx_0))],\n",
    "                        label_idx_1[int(0.6 * len(label_idx_1)):int(0.8 * len(label_idx_1))])\n",
    "    idx_test = np.append(label_idx_0[int(0.8 * len(label_idx_0)):], label_idx_1[int(0.8 * len(label_idx_1)):])\n",
    "\n",
    "    train_mask = torch.from_numpy(sample_mask(idx_train, node_features.shape[0]))\n",
    "    val_mask = torch.from_numpy(sample_mask(idx_val, node_features.shape[0]))\n",
    "    test_mask = torch.from_numpy(sample_mask(idx_test, node_features.shape[0]))\n",
    "\n",
    "    return aug_adj, features, labels, sensitive_attribute, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bail_multi dataset...\n",
      "       ALCHY  JUNKY  SUPER  MARRIED  FELON  WORKREL  PROPTY  PERSON  PRIORS  \\\n",
      "0          0      0      1        0      0        0       0       0       1   \n",
      "1          0      0      1        0      0        0       0       0       2   \n",
      "2          1      0      1        1      0        1       0       0       0   \n",
      "3          0      0      1        1      0        0       0       0       0   \n",
      "4          0      0      0        0      0        1       0       0       0   \n",
      "...      ...    ...    ...      ...    ...      ...     ...     ...     ...   \n",
      "18871      0      0      1        1      0        1       1       0       0   \n",
      "18872      0      0      1        1      1        1       0       0       0   \n",
      "18873      0      1      1        0      0        0       0       0       3   \n",
      "18874      0      0      1        0      1        1       0       0       3   \n",
      "18875      0      1      0        0      0        0       0       0       0   \n",
      "\n",
      "       SCHOOL  RULE  TSERVD  FOLLOW  TIME  FILE  \n",
      "0           8     0       1      81     0     3  \n",
      "1          11     0       1      80     0     3  \n",
      "2           7     2      30      72     0     1  \n",
      "3          11     0       4      81     0     2  \n",
      "4           0     3      13      76     0     3  \n",
      "...       ...   ...     ...     ...   ...   ...  \n",
      "18871      12     0      10      53     0     2  \n",
      "18872      14     0       3      56     0     1  \n",
      "18873      10     1      16      55    48     2  \n",
      "18874       0     0       7      53     0     3  \n",
      "18875      10     3       7      50     0     2  \n",
      "\n",
      "[18876 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"bail_multi\"\n",
    "lr = 0.0001\n",
    "weight_decay = 1e-5\n",
    "dropout = 0.0\n",
    "epochs = 1000\n",
    "seed = 10\n",
    "hidden_dim = 512\n",
    "path = \"data/\"\n",
    "\n",
    "reg_lambda=5e-4\n",
    "\n",
    "loss_lt= []\n",
    "loss_adv_lt = []\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "adj, features, labels, gender, train_mask, val_mask, test_mask= load_data(path=path, dataset=dataset, seed=seed)\n",
    "\n",
    "train_mask_1 = (gender[train_mask] == 2) | (gender[train_mask]==3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T05:06:39.120718Z",
     "start_time": "2024-12-30T05:06:38.825423Z"
    }
   },
   "id": "eb0edb9f0b4d65c"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class_seen = torch.from_numpy(np.load(path+\"bail_truth_class_seen.npy\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T05:06:39.896431Z",
     "start_time": "2024-12-30T05:06:39.890334Z"
    }
   },
   "id": "26325c1530b31496"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5304600082884376, 0.5632732797916215, 0.5793614067561315, 0.6431902985074627]\n",
      "[0.0, 0.0, 1.0, 1.0]\n",
      "Epoch:  0 loss:  1.392905354499817 acc : 0.5744812362030906 seen/unseen acc:  0.3801324503311258\n",
      "Confusion Matrix class: \n",
      " [[6119  944]\n",
      " [3875  387]]\n",
      "Confusion Matrix sens: \n",
      " [[4305    0]\n",
      " [7020    0]]\n",
      "Loss class:  0.6929842233657837 Loss unseen:  0.6999211311340332 Loss Parity:  0.0003268718719482422\n",
      "[0.7612929962702031, 0.8484914260907315, 0.8514576584914392, 0.8325559701492538]\n",
      "[0.9912971404890178, 0.9350987627523334, 0.8496066635816751, 0.992070895522388]\n",
      "Epoch:  200 loss:  0.867132306098938 acc : 0.8274613686534217 seen/unseen acc:  0.9415452538631347\n",
      "Confusion Matrix class: \n",
      " [[7013   50]\n",
      " [1904 2358]]\n",
      "Confusion Matrix sens: \n",
      " [[3963  342]\n",
      " [ 320 6700]]\n",
      "Loss class:  0.4948713481426239 Loss unseen:  0.3722609579563141 Loss Parity:  0.07431697845458984\n",
      "[0.7592208868628264, 0.8903841979596266, 0.9361406756131421, 0.9132462686567164]\n",
      "[0.9908827186075425, 0.9546342522248752, 0.895881536325775, 0.9986007462686567]\n",
      "Epoch:  400 loss:  0.7429975271224976 acc : 0.8754966887417218 seen/unseen acc:  0.9594701986754967\n",
      "Confusion Matrix class: \n",
      " [[6079  984]\n",
      " [ 426 3836]]\n",
      "Confusion Matrix sens: \n",
      " [[4077  228]\n",
      " [ 231 6789]]\n",
      "Loss class:  0.3893831968307495 Loss unseen:  0.35361433029174805 Loss Parity:  0.2672426700592041\n",
      "[0.743058433485288, 0.8953766008248317, 0.9430819065247571, 0.9216417910447762]\n",
      "[0.9896394529631165, 0.964401996961146, 0.9264229523368811, 0.9990671641791045]\n",
      "Epoch:  600 loss:  0.7241288423538208 acc : 0.8769977924944813 seen/unseen acc:  0.9690949227373068\n",
      "Confusion Matrix class: \n",
      " [[6085  978]\n",
      " [ 415 3847]]\n",
      "Confusion Matrix sens: \n",
      " [[4144  161]\n",
      " [ 189 6831]]\n",
      "Loss class:  0.37969598174095154 Loss unseen:  0.34443289041519165 Loss Parity:  0.27661728858947754\n",
      "[0.7252382925818484, 0.8569568048621663, 0.950485886163813, 0.929570895522388]\n",
      "[0.9900538748445918, 0.9693943998263512, 0.950485886163813, 0.9990671641791045]\n",
      "Epoch:  800 loss:  0.7126885652542114 acc : 0.860485651214128 seen/unseen acc:  0.9758057395143488\n",
      "Confusion Matrix class: \n",
      " [[5843 1220]\n",
      " [ 360 3902]]\n",
      "Confusion Matrix sens: \n",
      " [[4196  109]\n",
      " [ 165 6855]]\n",
      "Loss class:  0.3736642897129059 Loss unseen:  0.33902427554130554 Loss Parity:  0.2882070541381836\n",
      "---------------EVALUATION------Alpha:  0  Beta:  0  seed:  10 --------------------------------\n",
      "|*|Test: acc : 0.8421610169491526 || Auc layer: 0.9227558715117938 || AUC Male: 0.9515940888299075 || AUC Female: 0.833684519704748 || SP: 0.20672442311786576 || EQ: 0.012751131861912901|*|\n",
      "-----------------------------------------------------\n",
      "[0.7049180327868853, 0.8324675324675325, 0.9393139841688655, 0.9153284671532846]\n",
      "[0.833684519704748, 0.9515940888299075, 0.9422335319430653, 0.9351326887880959]\n",
      "-----------------------------------------------------\n",
      "|*|Test: acc : 0.8421610169491526 || Auc layer: 0.9227558715117938 || AUC Max: 0.9515940888299075 || AUC min: 0.833684519704748 || SP: 0.44643932580947876 || EQ: 0.10589883386339549|*|\n",
      "[0.9962168978562421, 0.962987012987013, 0.9643799472295514, 0.997080291970803]\n",
      "Error with calculating fairness metrics\n"
     ]
    }
   ],
   "source": [
    "#lt_alpha = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "lt_alpha = [0,0.001,0.010,0.10]\n",
    "#lt_alpha = [0.5,0.7,0.9]\n",
    "#lt_alpha = [0]\n",
    "#lt_beta = [0]\n",
    "lt_beta = [0,0.5,1,2,5,10,50]\n",
    "layers = [128,128,8]\n",
    "seeds = [10]\n",
    "for seed in seeds:\n",
    "    for beta in lt_beta:\n",
    "        loss_ce_class = []\n",
    "        loss_ce_seen = []\n",
    "        loss_ce_sens = []\n",
    "        loss_ce_dis_1 = []\n",
    "        loss_ce_dis_2 = []\n",
    "        total_loss_lt = []\n",
    "        for alpha in lt_alpha:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            model_encoder = GCN(features.shape[1], hidden_dim, 128, dropout=dropout)\n",
    "            model_encoder_dnn = DNN(layers=layers,dropout=0.0)\n",
    "            model_param_layer = ParamLayer(4,4)\n",
    "            optimizer = torch.optim.Adam(model_encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            optimizer_dnn = torch.optim.Adam(model_encoder_dnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            #criterion_unseen = nn.CrossEntropyLoss(weight=torch.tensor([1.0,100.0]))\n",
    "            \n",
    "            model_class = DNN(layers=[4,128, 128,2],dropout=0.0)\n",
    "            optimizer_class = torch.optim.Adam(model_class.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            \n",
    "            \n",
    "            model_unseen = DNN(layers=[4,128,128,2],dropout=0.0)\n",
    "            optimizer_unseen = torch.optim.Adam(model_unseen.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            optimizer_sens = torch.optim.Adam(model_param_layer.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                model_encoder.train()\n",
    "                optimizer.zero_grad()\n",
    "                optimizer_dnn.zero_grad()\n",
    "                optimizer_class.zero_grad()\n",
    "                optimizer_unseen.zero_grad()\n",
    "                optimizer_sens.zero_grad()\n",
    "                \n",
    "                #encoder\n",
    "                temp_emb = model_encoder(features,adj)\n",
    "                emb = model_encoder_dnn(temp_emb)\n",
    "            \n",
    "                \n",
    "                #sensitive attribute val prediction\n",
    "                \n",
    "                #decoder\n",
    "                #feat_pred = model_decoder_feat(emb)\n",
    "                #adj_pred = model_decoder_adj(emb)\n",
    "                \n",
    "                \n",
    "                #mse_loss_feat = torch.sum((feat_pred[train_mask]-features[train_mask])**2)\n",
    "                #mse_loss_adj = torch.sum((adj_pred.to_sparse()-adj)**2)\n",
    "                #loss =  mse_loss_adj + mse_loss_feat\n",
    "                \n",
    "                #classifier\n",
    "                \n",
    "                class_emb = F.softmax(model_class(emb[:,4:].detach()),dim=1)\n",
    "                loss_class =  criterion(class_emb[train_mask][train_mask_1],labels[train_mask][train_mask_1])\n",
    "                loss_class.backward()\n",
    "                optimizer_class.step()\n",
    "            \n",
    "                \n",
    "                #sensitive attribute prediction unseen & seen\n",
    "                #param_emb_mul = model_param_layer(emb[:,:4].detach())\n",
    "                unseen_emb = F.softmax(model_unseen(emb[:,:4].detach()),dim=1)\n",
    "                loss_unseen = criterion(unseen_emb,class_seen)\n",
    "                loss_unseen.backward()\n",
    "                optimizer_unseen.step()\n",
    "                optimizer_sens.step()\n",
    "                #corr\n",
    "                #corr_att = torch.cat([torch.flatten(emb[:,2:][train_mask]).unsqueeze(1),torch.flatten(emb[:,:2][train_mask]).unsqueeze(1)],dim=1)\n",
    "                #corr_matrix = torch.corrcoef(corr_att)\n",
    "                #corr_v = abs(corr_matrix[0,1])\n",
    "                #print('here')\n",
    "                #different losses\n",
    "                class_emb = F.softmax(model_class(emb[:,4:]),dim=1)\n",
    "                loss_class =  criterion(class_emb[train_mask][train_mask_1],labels[train_mask][train_mask_1])\n",
    "                loss_ce_class.append(loss_class.item())\n",
    "                \n",
    "                #param_emb_mul = model_param_layer(emb[:,:4].detach())\n",
    "                unseen_emb = F.softmax(model_unseen(emb[:,:4]),dim=1)\n",
    "                loss_sens = criterion(unseen_emb[train_mask],class_seen[train_mask])\n",
    "                loss_ce_seen.append(loss_sens.item())\n",
    "                \n",
    "                #sensitive attribute val prediction\n",
    "                #sens_val_emb =  F.softmax(emb[:,:4])\n",
    "                #loss_sens_val =  criterion(sens_val_emb[train_mask][train_mask_1],gender[train_mask][train_mask_1])\n",
    "                #loss_ce_sens.append(loss_sens_val.item())\n",
    "                \n",
    "                #class_emb_neg = F.softmax(model_class(emb[:,:4]),dim=1)\n",
    "                #loss_class_neg = criterion(class_emb_neg[train_mask],labels[train_mask])\n",
    "                \n",
    "                #unseen_emb_neg =  F.softmax(model_unseen(emb[:,4:]),dim=1)\n",
    "                #loss_sens_neg =  criterion(unseen_emb_neg[train_mask],class_seen[train_mask])\n",
    "    \n",
    "                #loss_ce_dis_1.append(loss_class_neg.item())\n",
    "                #loss_ce_dis_2.append(loss_sens_neg.item())\n",
    "                \n",
    "                #preds_class = class_emb.max(1)[1].type_as(labels)\n",
    "                \n",
    "                idx_s0 = gender== 0\n",
    "                idx_s1 = gender == 1\n",
    "                idx_s2 = class_seen == 0\n",
    "            \n",
    "                #parity = abs(sum(F.sigmoid(class_emb[idx_s0][:,1])*3) / sum(labels[idx_s0]) - sum(F.sigmoid(class_emb[idx_s1][:,1])*3) / sum(labels[idx_s1]))\n",
    "                #parity_1 = abs((sum(F.sigmoid(class_emb[idx_s0][:,1])*3) / sum(idx_s0)) - (sum(F.sigmoid(class_emb[idx_s1][:,1])*3) / sum(idx_s1)))\n",
    "                parity_2 = abs((sum(F.sigmoid(class_emb[idx_s1][:,1])*3) / sum(idx_s1)) - (sum(F.sigmoid(class_emb[idx_s2][:,1])*3) / sum(idx_s2)))\n",
    "                parity_3 = abs((sum(F.sigmoid(class_emb[idx_s0][:,1])*3) / sum(idx_s0)) - (sum(F.sigmoid(class_emb[idx_s2][:,1])*3) / sum(idx_s2)))\n",
    "                #parity=max(parity_1,parity_2,parity_3)\n",
    "                parity=max(parity_2,parity_3)\n",
    "                \n",
    "                #vector_class = emb[:,4:].reshape(-1)\n",
    "                #vector_unseen = emb[:,:4].reshape(-1)\n",
    "                #vector_class= vector_class.view(vector_class.shape[0], 1)\n",
    "                #vector_unseen= vector_unseen.view(vector_unseen.shape[0], 1)\n",
    "                #print(vector_class.shape)\n",
    "                #print(vector_unseen.shape)\n",
    "                #vector_both = torch.cat((vector_unseen.T,vector_class.T),dim=0)\n",
    "                #print(vector_both.shape)\n",
    "                #coef = torch.abs(torch.corrcoef(vector_both)[0][1])\n",
    "                coef = torch.abs(torch.corrcoef(emb.T))\n",
    "                #print(coef)\n",
    "                #print(coef[:4,4:])\n",
    "                coef = torch.max(coef[:4,4:])\n",
    "                #print(coef)\n",
    "                \n",
    "                \n",
    "                total_loss = (1-alpha)*(loss_class+loss_sens)+alpha*(coef) + beta*parity\n",
    "                #total_loss = (1-alpha)*(loss_class+beta*loss_sens+loss_sens_val)-alpha*(loss_class_neg+ loss_sens_neg) + beta*parity\n",
    "                \n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer_dnn.step()\n",
    "                optimizer_unseen.step()\n",
    "                optimizer_sens.step()\n",
    "                optimizer_class.step()\n",
    "                \n",
    "                if epoch % 200 == 0:\n",
    "                    #print(\"Epoch: \", epoch,\"loss: \", total_loss.item() )\n",
    "                    preds_1 = class_emb.max(1)[1].type_as(labels)\n",
    "                    acc = acc_measurements_multi(class_emb[train_mask], labels[train_mask], gender[train_mask])\n",
    "                    acc_unseen = acc_measurements_multi(unseen_emb[train_mask], class_seen[train_mask], gender[train_mask])\n",
    "                    print(\"Epoch: \", epoch,\"loss: \", total_loss.item(),  \"acc :\",acc[0] ,\"seen/unseen acc: \", acc_unseen[0])\n",
    "                    print(\"Confusion Matrix class: \\n\",confusion_matrix(labels[train_mask], preds_1[train_mask]))\n",
    "                    preds_1 = unseen_emb.max(1)[1].type_as(labels)\n",
    "                    print(\"Confusion Matrix sens: \\n\",confusion_matrix(class_seen[train_mask], preds_1[train_mask]))\n",
    "                    print(\"Loss class: \",loss_class.item(),\"Loss unseen: \",loss_sens.item(), \"Loss Parity: \" ,parity.item())\n",
    "                    \n",
    "    \n",
    "            temp_emb = model_encoder(features, adj)\n",
    "            emb = model_encoder_dnn(temp_emb)\n",
    "            class_emb = F.softmax(model_class(emb[:, 4:]), dim=1)\n",
    "            unseen_emb = F.softmax(model_unseen(emb[:, :4]), dim=1)\n",
    "            \n",
    "            preds_class = class_emb.max(1)[1].type_as(labels)\n",
    "            preds_sens = unseen_emb.max(1)[1].type_as(labels)\n",
    "            \n",
    "            acc_test = acc_measurements(class_emb[test_mask], labels[test_mask], gender[test_mask])\n",
    "            auc_roc_test, auc_m, auc_f = auc_measurements(class_emb[test_mask], labels[test_mask],\n",
    "                                                          gender[test_mask])\n",
    "            \n",
    "            parity, equality = fair_metric(preds_class[test_mask].numpy(), labels[test_mask].numpy(),\n",
    "                                           gender[test_mask].numpy())\n",
    "            print('---------------EVALUATION------Alpha: ',alpha,' Beta: ',beta,' seed: ', seed,'--------------------------------')\n",
    "            lt = [acc_test[0], auc_roc_test, auc_m, auc_f, parity, equality]\n",
    "            print(\n",
    "                f'|*|Test: acc : {acc_test[0]} || Auc layer: {auc_roc_test} || AUC Male: {auc_m} || AUC Female: {auc_f} || SP: {parity} || EQ: {equality}|*|')\n",
    "            print('-----------------------------------------------------')\n",
    "            try:\n",
    "                acc_test = acc_measurements_multi(class_emb[test_mask], labels[test_mask], gender[test_mask])\n",
    "                auc_m, auc_f = auc_measurements_multi(class_emb[test_mask], labels[test_mask], gender[test_mask])\n",
    "                index, parity, equality = fair_metric_multi(preds_class[test_mask].numpy(), labels[test_mask].numpy(),\n",
    "                                                            gender[test_mask])\n",
    "                \n",
    "                print('-----------------------------------------------------')\n",
    "                lt = [acc_test[0], auc_roc_test, auc_m, auc_f, parity, equality]\n",
    "                print(\n",
    "                    f'|*|Test: acc : {acc_test[0]} || Auc layer: {auc_roc_test} || AUC Max: {auc_m} || AUC min: {auc_f} || SP: {parity} || EQ: {equality}|*|')\n",
    "                \n",
    "                acc_test = acc_measurements_multi(unseen_emb[test_mask], class_seen[test_mask], gender[test_mask])\n",
    "                auc_roc_test, auc_m, auc_f = auc_measurements(unseen_emb[test_mask], class_seen[test_mask],\n",
    "                                                          gender[test_mask])\n",
    "                print(\n",
    "                    f'|*|Test sens: acc : {acc_test[0]} || Auc layer: {auc_roc_test} || AUC Max: {auc_m} || AUC min: {auc_f}')\n",
    "                print(\"correlation\",coef)\n",
    "                print('-----------------------------------------------------')\n",
    "            except:\n",
    "                print(\"Error with calculating fairness metrics\")\n",
    "            \n",
    "            str_model = \"param_saved/bail_december_2_3_only/bail_model_encoder_alpha_\" + str(int(alpha*10000))+\"_beta_\"+str(int(beta*10))+\"_seed_\"+str(int(seed))+\".pth\"\n",
    "            torch.save(model_encoder.state_dict(), str_model)\n",
    "            str_model = \"param_saved/bail_december_2_3_only/bail_model_encoder_dnn_alpha_\" + str(int(alpha*10000))+\"_beta_\"+str(int(beta*10))+\"_seed_\"+str(int(seed))+\".pth\"\n",
    "            torch.save(model_encoder_dnn.state_dict(), str_model)\n",
    "            str_model = \"param_saved/bail_december_2_3_only/bail_model_class_alpha_\" + str(int(alpha*10000))+\"_beta_\"+str(int(beta*10))+\"_seed_\"+str(int(seed))+\".pth\"\n",
    "            torch.save(model_class.state_dict(), str_model)\n",
    "            str_model = \"param_saved/bail_december_2_3_only/bail_model_unseen_alpha_\" + str(int(alpha*10000))+\"_beta_\"+str(int(beta*10))+\"_seed_\"+str(int(seed))+\".pth\"\n",
    "            torch.save(model_unseen.state_dict(), str_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T05:15:30.678748Z",
     "start_time": "2024-12-30T05:06:47.242616Z"
    }
   },
   "id": "ad488e415196e866"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def fair_metric_multi(pred, labels, sens):\n",
    "    parity_lt = []\n",
    "    eq_lt = []\n",
    "    index = []\n",
    "    for i in torch.unique(sens):\n",
    "        idx_s0 = sens == i\n",
    "        # idx_s0_y1 = np.bitwise_and(idx_s0, labels == 1)\n",
    "        tn, fp, fn, tp = confusion_matrix(labels[idx_s0], pred[idx_s0]).ravel()\n",
    "        sp_i = sum(pred[idx_s0]) / sum(idx_s0)\n",
    "        eq_i = tp / sum(labels[idx_s0])\n",
    "        for j in torch.unique(sens):\n",
    "            if i != j:\n",
    "                idx_s1 = sens == j\n",
    "                tn_j, fp_j, fn_j, tp_j = confusion_matrix(labels[idx_s1], pred[idx_s1]).ravel()\n",
    "                sp_j = sum(pred[idx_s1]) / sum(idx_s1)\n",
    "                eq_j = tp_j / sum(labels[idx_s1])\n",
    "\n",
    "                parity = abs(sp_i - sp_j)\n",
    "                equality = abs(eq_i - eq_j)\n",
    "\n",
    "                index.append([i, j])\n",
    "                parity_lt.append(parity.item())\n",
    "                eq_lt.append(equality.item())\n",
    "    max_ = max(parity_lt)\n",
    "    parity_index = parity_lt.count(max_)\n",
    "    max_eq = max(eq_lt)\n",
    "    eq_index = eq_lt.count(max_eq)\n",
    "    print('index of sp: ', index[parity_index])\n",
    "    print('index of eq: ', index[eq_index])\n",
    "    # print(parity_lt)\n",
    "    # print(eq_lt)\n",
    "    return index, max_, max_eq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-21T00:35:14.866914Z",
     "start_time": "2024-12-21T00:35:14.864807Z"
    }
   },
   "id": "1fff792e722f6dca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_lt = []\n",
    "auc_lt = []\n",
    "sp_lt =[]\n",
    "eo_lt = []\n",
    "model_encoder = GCN(features.shape[1], hidden_dim, 128, dropout=dropout)\n",
    "model_encoder_dnn = DNN(layers=layers,dropout=0.0)\n",
    "model_class = DNN(layers=[4,128, 128,2],dropout=0.0)\n",
    "model_unseen = DNN(layers=[4,128,128,2],dropout=0.0)\n",
    "lt_alpha= [1.0]\n",
    "for alpha in lt_alpha:\n",
    "    str_model = \"param_saved/tagged_eo_reg_alldata/tagged_model_encoder_alpha_\" + str(int(alpha*10))+\".pth\"\n",
    "    model_encoder.load_state_dict(torch.load(str_model))\n",
    "    str_model = \"param_saved/tagged_eo_reg_alldata/tagged_model_encoder_dnn_alpha_\" + str(int(alpha*10))+\".pth\"\n",
    "    model_encoder_dnn.load_state_dict(torch.load(str_model))\n",
    "    str_model = \"param_saved/tagged_eo_reg_alldata/tagged_model_class_alpha_\" + str(int(alpha*10))+\".pth\"\n",
    "    model_class.load_state_dict(torch.load(str_model))\n",
    "    str_model = \"param_saved/tagged_eo_reg_alldata/tagged_model_unseen_alpha_\" + str(int(alpha*10))+\".pth\"\n",
    "    model_unseen.load_state_dict(torch.load(str_model))\n",
    "    \n",
    "    temp_emb = model_encoder(features, adj)\n",
    "    emb = model_encoder_dnn(temp_emb)\n",
    "    class_emb = F.softmax(model_class(emb[:, 4:]), dim=1)\n",
    "    unseen_emb = F.softmax(model_unseen(emb[:, :4]), dim=1)\n",
    "    \n",
    "    preds_class = class_emb.max(1)[1].type_as(labels)\n",
    "    preds_sens = unseen_emb.max(1)[1].type_as(labels)\n",
    "    \n",
    "    acc_test = acc_measurements(class_emb[test_mask], labels[test_mask], gender[test_mask])\n",
    "    auc_roc_test, auc_m, auc_f = auc_measurements(class_emb[test_mask], labels[test_mask],\n",
    "                                                  gender[test_mask])\n",
    "    \n",
    "    parity, equality = fair_metric(preds_class[test_mask].numpy(), labels[test_mask].numpy(),\n",
    "                                   gender[test_mask].numpy())\n",
    "    print('---------------EVALUATION----------------Alpha: ',alpha,'-------------------')\n",
    "    lt = [acc_test[0], auc_roc_test, auc_m, auc_f, parity, equality]\n",
    "    #print(\n",
    "    #    f'|*|Test: acc : {acc_test[0]} || Auc layer: {auc_roc_test} || AUC Male: {auc_m} || AUC Female: {auc_f} || SP: {parity} || EQ: {equality}|*|')\n",
    "    print('-----------------------------------------------------')\n",
    "    acc_test = acc_measurements_multi(class_emb[test_mask], labels[test_mask], gender[test_mask])\n",
    "    auc_m, auc_f = auc_measurements_multi(class_emb[test_mask], labels[test_mask], gender[test_mask])\n",
    "    index, parity, equality = fair_metric_multi(preds_class[test_mask].numpy(), labels[test_mask].numpy(),\n",
    "                                                gender[test_mask])\n",
    "    f1_s = f1_score(labels[test_mask].numpy(), preds_class[test_mask].detach().numpy())\n",
    "    \n",
    "    f1_lt.append(f1_s)\n",
    "    auc_lt.append(auc_roc_test)\n",
    "    sp_lt.append(parity)\n",
    "    eo_lt.append(equality)\n",
    "    \n",
    "    print('-----------------------------------------------------')\n",
    "    lt = [acc_test[0], auc_roc_test, auc_m, auc_f, parity, equality]\n",
    "    print(\n",
    "        f'|*|Test: acc : {acc_test[0]} || f1-score:{f1_s} Auc layer: {auc_roc_test} || AUC Max: {auc_m} || AUC min: {auc_f} || SP: {parity} || EQ: {equality}|*|')\n",
    "    print('-----------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-31T18:19:53.774764Z"
    }
   },
   "id": "b83733ba9f8ba0c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "85b1a9cba1452ec6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
